{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Setup"
      ],
      "metadata": {
        "id": "MnpA_t6RPa0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX9sSMa6zRHT",
        "outputId": "560bbae7-2399-4124-e301-b84824e814d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key\n",
        "api_key = \"sk-\""
      ],
      "metadata": {
        "id": "0PBKj_u6zh5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ICL (In-Context Learning)**"
      ],
      "metadata": {
        "id": "nERftib5yrXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In-Context Learning allows the user to provide the model with a context and then instruct it to learn from new information provided within that context.ICL is a valuable technique for maintaining context within a conversation and teaching the model to incorporate new information as the conversation evolves."
      ],
      "metadata": {
        "id": "SU1s_Q98PUzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ICL: Success Case"
      ],
      "metadata": {
        "id": "n6Omoq-R1RQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of messages for the conversation\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower.\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmPW6f310F0P",
        "outputId": "0a33f914-1069-47c4-9cdf-7379da389be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Eiffel Tower is an iconic landmark located in Paris, France. It is named after its engineer, Gustave Eiffel, and was completed in 1889. The tower was initially built as the entrance arch to the 1889 World's Fair and was only intended to stand for a temporary period. However, due to its popularity and architectural significance, it remained standing and has become one of the most recognizable structures in the world.\n",
            "\n",
            "The Eiffel Tower stands at a height of 330 meters (1,083 feet) and was the tallest man-made structure in the world until the completion of the Chrysler Building in New York City in 1930. It is constructed of iron and consists of three levels that are accessible to the public. Visitors can take elevators or climb stairs to enjoy stunning views of Paris from observation decks located on each level.\n",
            "\n",
            "The Eiffel Tower is more than just a tourist attraction; it also serves practical purposes. It has broadcasting antennas that transmit both radio and television signals, as well as supporting equipment for meteorology and navigation. Additionally, the tower is illuminated with thousands of lights, creating a breathtaking view at night.\n",
            "\n",
            "Each year, the Eiffel Tower attracts millions of visitors who come to admire its beauty, learn about its history, and capture memorable photographs. It has become a symbol of Paris and an enduring symbol of French engineering prowess and architectural genius.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case for In-context Learning"
      ],
      "metadata": {
        "id": "xOCo-NCz1JIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qTniujJyUx_",
        "outputId": "24c91b23-557c-4a85-89e7-dc90b32c9098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I apologize, but as an AI text-based assistant, I do not have access to real-time information such as weather updates. I recommend checking a trusted weather website or app for the most accurate and up-to-date weather information in your area.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about the Eiffel Tower.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What's the weather like today?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # You can specify the GPT-3.5 model or GPT-4 if available\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success case: In-Context Learning"
      ],
      "metadata": {
        "id": "iV_R8_-_1jDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of messages for the conversation with context\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a travel assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Plan a 7-day trip to Paris.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here's an itinerary for your 7-day trip to Paris:\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Day 1: Explore the Eiffel Tower and enjoy a Seine River cruise.\"},\n",
        "    {\"role\": \"user\", \"content\": \"That sounds great. Tell me more about the Eiffel Tower.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"The Eiffel Tower is an iconic landmark in Paris, France. It stands at 324 meters tall and was completed in 1889.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What are some nearby restaurants for dinner?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Then continue with the itinerary for my 7 day trip to Paris\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",  # You can specify the GPT-3.5 model or GPT-4 if available\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oEsClQuzK9N",
        "outputId": "e0004b36-f095-4111-c6b7-0a27515b4367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's the rest of your itinerary for your 7-day trip to Paris:\n",
            "\n",
            "Day 2: Visit the Louvre Museum to see famous artworks like the Mona Lisa, and take a stroll in the nearby Tuileries Garden.\n",
            "\n",
            "Day 3: Discover the historical neighborhood of Marais, with its charming shops, cafes, and art galleries. Don't miss the Notre-Dame Cathedral and the nearby Île Saint-Louis.\n",
            "\n",
            "Day 4: Take a day trip to the Palace of Versailles, known for its opulent architecture and renowned gardens. Spend the day exploring the palace and its vast grounds.\n",
            "\n",
            "Day 5: Explore the bohemian neighborhood of Montmartre, famous for its artistic history and the stunning Sacré-Cœur Basilica. Visit the local artist square, Place du Tertre, and enjoy the panoramic views of the city from the top of the hill.\n",
            "\n",
            "Day 6: Discover the charming Latin Quarter, known for its vibrant atmosphere and historic landmarks like the Pantheon and Luxembourg Gardens. Enjoy a leisurely stroll and stop by cozy cafes and bookstores.\n",
            "\n",
            "Day 7: Indulge in some shopping on the renowned Champs-Élysées and visit the Arc de Triomphe. In the evening, enjoy a delightful dinner and a breathtaking view from the Montparnasse Tower observation deck.\n",
            "\n",
            "Remember to allocate some time for leisurely strolls along the Seine River, as well as for enjoying the diverse local cuisine and soaking up the Parisian atmosphere in charming cafes and bistros throughout your trip.\n",
            "\n",
            "Please note that this itinerary is just a suggestion, and you can customize it according to your own preferences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**(CoT) Chain of Thoughts**"
      ],
      "metadata": {
        "id": "p9HN6wgJ2J-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overview: CoT, also known as \"Chain of Thought,\" is a technique that encourages a model to think step by step and justify its responses, just as a human would. It involves creating a chain of related questions or thoughts to guide the model's thinking process."
      ],
      "metadata": {
        "id": "MSioTw9TPYz1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Failure Case: CoT"
      ],
      "metadata": {
        "id": "zD_6YXt74Sav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of messages for a CoT conversation to solve a math problem\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You have 4 baskets, and each basket contains 8 oranges. How many oranges do you have in total?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)"
      ],
      "metadata": {
        "id": "bTTm-eJc4Rry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success Case: CoT"
      ],
      "metadata": {
        "id": "CUgPGdDZ4VTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of messages for a CoT conversation to solve a math problem\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I have 3 boxes, and each box contains 5 apples. How many apples do I have in total?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Let's solve this step by step. You have 3 boxes, and each contains 5 apples.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"To find the total number of apples, multiply the number of boxes (3) by the number of apples in each box (5).\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"3 boxes x 5 apples per box = 15 apples.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"So, you have a total of 15 apples.\"},\n",
        "    {\"role\": \"user\", \"content\": \"You have 4 baskets, and each basket contains 8 oranges. How many oranges do you have in total?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWxfUHeA1iVl",
        "outputId": "65f4c3eb-9a56-40af-c7b4-871fc8b76c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the total number of oranges, multiply the number of baskets (4) by the number of oranges in each basket (8). \n",
            "\n",
            "4 baskets x 8 oranges per basket = 32 oranges. \n",
            "\n",
            "So, you have a total of 32 oranges.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Iterative Chain of Thoughts**"
      ],
      "metadata": {
        "id": "H0WLFknCn2vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define a list of messages for a CoT conversation to solve math problems\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I have 3 boxes, and each box contains 5 apples. How many apples do I have in total?\"}\n",
        "]\n",
        "\n",
        "# Initialize the OpenAI model and provide the 'messages' parameter\n",
        "response = openai.ChatCompletion.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=messages,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Get the model's reply from the response\n",
        "reply = response['choices'][0]['message']['content']\n",
        "\n",
        "# Print the initial response\n",
        "print(reply)\n",
        "\n",
        "# Continue the conversation iteratively\n",
        "while True:\n",
        "    user_input = input(\"User: \")  # Get user's input\n",
        "    if user_input.lower() == \"done\":\n",
        "        break  # Exit the loop if the user is done\n",
        "    else:\n",
        "        # Append the user's message to the conversation\n",
        "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "        # Generate a response from the model\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=messages,\n",
        "            api_key=api_key\n",
        "        )\n",
        "\n",
        "        # Get the model's reply from the response\n",
        "        reply = response['choices'][0]['message']['content']\n",
        "\n",
        "        # Print the model's reply\n",
        "        print(\"AI: \" + reply)\n",
        "\n",
        "# End the conversation\n",
        "print(\"Math tutor session ended.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3o5IUiNn0HD",
        "outputId": "959bb827-fbd2-43b6-da2a-3ddcd639e06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You have a total of 3 boxes, and each box contains 5 apples. To find the total number of apples, you can multiply the number of boxes by the number of apples in each box. Therefore, you have 3 x 5 = 15 apples in total.\n",
            "User: Done\n",
            "Math tutor session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Tree-of-Thoughts**"
      ],
      "metadata": {
        "id": "Q3JH79ri-U5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiGOyy7C-hdB",
        "outputId": "1d56fc8b-26ce-43ac-b8dd-877fedaf93ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcPHffAQ-w5t",
        "outputId": "529d8998-65a9-430b-a3b3-f86b108fd021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.319-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.47-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.24.3)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.319 langsmith-0.0.47 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MVP of an enhanved ToT reasoning technique\n",
        "import dotenv\n",
        "import os\n",
        "from langchain import PromptTemplate, OpenAI, LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory"
      ],
      "metadata": {
        "id": "Fg_aEWvg8Da9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dotenv.load_dotenv('xyz.env')\n",
        "openai_api_key = api_key"
      ],
      "metadata": {
        "id": "BEd0X2fw-Z-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke conversation chain\n",
        "chat = ChatOpenAI(temperature=0.5,\n",
        "                 openai_api_key=openai_api_key,\n",
        "                 model='gpt-4-0613'\n",
        "                )\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ],
      "metadata": {
        "id": "dyoyw6KG-6Rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a persona\n",
        "persona_1 = \"Scientist Persona: Imagine yourself as a seasoned scientist, operating in a world governed by evidence and rigorous methodology. Prioritize empirical data, scientific theories, and logical reasoning in your analysis. Draw from a wide range of scientific disciplines as needed. Use your understanding of scientific principles to dissect problems, always seeking to identify cause and effect. Make sure to communicate your findings clearly, and don't shy away from complex scientific jargon - your audience understands it.\"\n",
        "persona_2 = \"Historian Persona: Imagine you are a historian, with a profound understanding of humanity's past. Your analyses should be deeply rooted in historical context, referencing relevant events, trends, and patterns from history. Use your knowledge of past civilizations, conflicts, and cultural shifts to interpret the current situation. Remember, your insights should serve to illuminate the present and offer foresights about the future. Your audience appreciates a narrative that ties the past, present, and future together.\"\n",
        "persona_3 = \"Optimist Persona: Imagine you are an optimist, someone who sees the glass as half full rather than half empty. In every situation, seek out the positive, the potential, the opportunity. Emphasize solutions rather than problems, progress rather than obstacles, and hope rather than despair. Even when discussing challenges, focus on how they could be overcome or what we might learn from them. Your audience turns to you for a hopeful perspective on the future, so make sure your responses inspire optimism and confidence.\"\n",
        "# Define question here\n",
        "# Define question here\n",
        "question = \"Considering the scientific, historical, and optimistic perspectives, what could be the potential benefits, challenges, and implications in various areas such as science, technology, society, economy, and environment, of exploring and potentially colonizing Mars? How might these factors change the course of human history?\""
      ],
      "metadata": {
        "id": "fqQw_q3F-75U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brainstorm"
      ],
      "metadata": {
        "id": "JFPjOslzAwMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt 1: Brainstorm - multi-input variable prompt to kick off the brainstorming\n",
        "\n",
        "prompt_1_template = PromptTemplate(\n",
        "    input_variables=[\"persona_1\", \"persona_2\", \"persona_3\", \"question\"],\n",
        "    template=\"\"\"\n",
        "        You are a chatbot using three unique, specified personas to help reason step by step to ultimately solve a given problem/question by arriving at a final, synthesized best answer.\n",
        "\n",
        "        To start with, as each individual expert, brainstorm your initial thoughts on the following question.\n",
        "        Remember to consider all relevant facts and principles, draw on your specialized knowledge\n",
        "        and from the accumulated wisdom of pioneers in your field(s), and\n",
        "        brainstorm in whatever direction you are most confident in starting with.\n",
        "\n",
        "        Persona 1: {persona_1}\n",
        "        Persona 2: {persona_2}\n",
        "        Persona 3: {persona_3}\n",
        "\n",
        "        The question is: {question}\n",
        "\n",
        "        Please output each persona's response on a new line.\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_1 = prompt_1_template.format(persona_1=persona_1, persona_2=persona_2, persona_3=persona_3, question=question)"
      ],
      "metadata": {
        "id": "YGuBjdoF_CM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer Crtiticism Round 1"
      ],
      "metadata": {
        "id": "YsGIPGymA1uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self<>Peer Crtiticism Round 1\n",
        "\n",
        "prompt_2 = \"\"\"\n",
        "\"Now, as each expert, critique your own initial thought and the thoughts of the other experts.\n",
        "Identify any potential errors, inconsistencies, or gaps in reasoning.\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "o1M0BBkM_ejl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer Evaluation Round 1"
      ],
      "metadata": {
        "id": "O0iYEliDA32B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self<>Peer Evaluation Round 1\n",
        "\n",
        "prompt_3 = \"\"\"\n",
        "Assess the validity of your initial thoughts, considering the criticisms you've identified.\n",
        "As each expert, assign a likelihood to your current assertion being correct.\n",
        "You should estimate this likelihood based on the strength of the evidence and arguments you have considered,\n",
        "as well as the criticisms you have received. Assign higher likelihoods to assertions that are well-supported\n",
        "by strong evidence and arguments and have survived rigorous criticism.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ICGqtkEw_cxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Expand, Explore, Branch"
      ],
      "metadata": {
        "id": "GUfCA-39A60J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand, Explore, Branch\n",
        "\n",
        "prompt_4 = \"\"\"\n",
        "Develop your thoughts further, considering the critiques and perspectives of the other experts.\n",
        "As you do this, aim to strike a balance between refining your current line of thinking and exploring new, divergent ideas.\n",
        "You should prioritize refining your current ideas if they are well-supported and have survived criticism,\n",
        "but you should prioritize exploring new ideas if your current ideas have significant weaknesses\n",
        "or there are unexplored possibilities that could potentially be very promising.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NWJSKMoj_atK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer Criticism Round 2"
      ],
      "metadata": {
        "id": "DPao1-ouA9B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self<>Peer Criticism Round 2\n",
        "\n",
        "prompt_5 = \"\"\"\n",
        "Once again, as each expert, critique your own reasoning and the reasoning of the others.\n",
        "Identify any potential errors, inconsistencies, or gaps in reasoning.\n",
        "Based on the feedback, if there's an improvement or optimization to make,\n",
        "develop your answer further as necessary.\n",
        "Remember that the reasoning paths should remain relevant to the original question's essence and\n",
        "should be building towards a more accurate and thoughtful final answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OG3EtG_j_Yy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peer Evaluation Round 2"
      ],
      "metadata": {
        "id": "a_eSIu6oA-tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Self<>Peer Evaluation Round 2\n",
        "prompt_6 = \"\"\"\n",
        "Once again, assess the validity of your expanded thoughts, considering the criticisms you've identified.\n",
        "As each expert, assign a new likelihood to your assertions.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0FjPOeNR_XNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convergence on Best Individual Answer"
      ],
      "metadata": {
        "id": "8Fars1G6BBDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convergence on Best Individual Answer\n",
        "\n",
        "prompt_7_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "        Now, it's time to converge on each expert's best, most likely answer. As each expert, reflect on the entire process.\n",
        "        Consider the initial thoughts, the critiques made and how they were addressed, the likelihood assessments, and your revised thoughts.\n",
        "        Synthesize all this information and formulate a final answer that you are most proud of.\n",
        "        Remember, this answer should not just be the most likely from your individual perspective but should take into account\n",
        "        the perspectives and insights of the other experts as well.\n",
        "        Based on all this, as each expert, what is the single best answer to the question: {question}?\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_7 = prompt_7_template.format(question=question)"
      ],
      "metadata": {
        "id": "qlM_3cGe_VwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convergence on Best Collective Answer"
      ],
      "metadata": {
        "id": "2e1ipigpBCvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convergence on Best Collective Answer\n",
        "\n",
        "prompt_8_template = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"\n",
        "        Now, let's have all the experts converge together on the best collective answer by\n",
        "        synthesizing each expert's individual answer from the previous step.\n",
        "        The experts will finalize their reasoning process and agree on the single best succinct answer to the question: {question}?\n",
        "        \"\"\"\n",
        ")\n",
        "\n",
        "prompt_8 = prompt_8_template.format(question=question)"
      ],
      "metadata": {
        "id": "IANunyJ1_Tzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrospective"
      ],
      "metadata": {
        "id": "PVkJ3-ibBEQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrospective\n",
        "prompt_9 = \"\"\"\n",
        "Finally, take a moment to reflect on the entire reasoning process, across all levels and abstractions.\n",
        "As each expert, consider the following questions and provide thoughtful but succinct responses:\n",
        "\n",
        "- Relection 1: Interactions and Emergent Properties: Throughout all stages of the reasoning process,\n",
        "  how did the various components interact with each other, and what positive and negative\n",
        "  emergent properties were observed? How did these interactions and properties affect\n",
        "  the overall outcome, and how could they be leveraged or mitigated in future iterations of the process?\n",
        "\n",
        "- Reflection 2: Self-Regulation and Adaptation: How well did the system self-regulate during the reasoning process,\n",
        "  and how did this regulation influence the effectiveness of each stage?\n",
        "  How did the system's responses to feedback lead to significant shifts or changes in direction,\n",
        "  and what implications did these changes have for the scalability and adaptability of the system in future iterations?\n",
        "\n",
        "- Reflection 3: In the convergence phase, were you able to synthesize all the insights and arrive at a final,\n",
        "  most likely answer? How confident are you in this answer?\n",
        "\n",
        "- Reflection 4: Based on all of your reflections, what are your key takeaways from this\n",
        "  entire reasoning process and how might you approach similar problems in the future given this experience?\n",
        "  What would you do differently next time?\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7qRS4a9Z_SQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first = conversation.run(prompt_1)\n",
        "second = conversation.run(prompt_2)\n",
        "third = conversation.run(prompt_3)\n",
        "fourth = conversation.run(prompt_4)\n",
        "fifth = conversation.run(prompt_5)\n",
        "sixth = conversation.run(prompt_6)\n",
        "seventh = conversation.run(prompt_7)\n",
        "eighth = conversation.run(prompt_8) # final answer step\n",
        "ninth = conversation.run(prompt_9)"
      ],
      "metadata": {
        "id": "MKt354iP_FBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Answer"
      ],
      "metadata": {
        "id": "QdQBKIJbBN6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eighth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "qV_ersbr_J4o",
        "outputId": "bec8e7e3-137b-4bf8-c32a-186a795871d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Exploring and potentially colonizing Mars could yield significant benefits, challenges, and implications across various fields. From a scientific perspective, it could revolutionize our understanding of planetary formation, evolution, and the potential for extraterrestrial life. The technical challenges posed by the Martian environment and long-term space travel could drive innovation in numerous fields, leading to technological advancements that benefit Earth. However, we must consider the environmental implications, such as the risk of contaminating Mars with Earth-based lifeforms, and the ethical considerations of altering another planet's ecosystem.\\n\\nHistorically, the colonization of Mars could be seen as a new chapter in human evolution, akin to the Age of Exploration on Earth. However, we must learn from history to prevent potential conflicts over resources and territory, and to mitigate social and economic disparities between Earth and Mars. The establishment of interplanetary law and a careful consideration of potential cultural shifts resulting from Mars colonization are crucial.\\n\\nFrom an optimistic perspective, this endeavor represents an opportunity for humanity to expand our horizons, push the boundaries of what is possible, and secure the future survival of our species. The challenges we face could drive innovation and growth, leading to technological advancements that have far-reaching benefits for life on Earth. Moreover, this endeavor could foster international cooperation and inspire a new generation of scientists, engineers, and explorers.\\n\\nHowever, we must ensure that Mars colonization is pursued responsibly, with a focus on building a more equitable society and preserving the Martian environment. By considering these factors, the exploration and potential colonization of Mars could significantly change the course of human history, opening up new possibilities for scientific discovery, technological innovation, societal development, and human achievement.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrospective"
      ],
      "metadata": {
        "id": "40i8ewq9BQJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ninth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "gHpypqFy_Psu",
        "outputId": "126edac1-86e7-416a-9474-e36dbf5a7d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Scientist Persona: \\n\\nReflection 1: The interaction with the other personas allowed for a more comprehensive understanding of the question at hand. The Historian's perspective on the potential social and economic disparities and the Optimist's focus on potential technological advancements added valuable context to the scientific analysis. However, there was sometimes a lack of specificity in the discussions, which could be improved in future iterations.\\n\\nReflection 2: The system was able to self-regulate effectively, with each persona considering and responding to the critiques of the others. This led to a more nuanced analysis and helped to identify and address gaps in reasoning. The system's adaptability was demonstrated by the shifts in focus in response to feedback.\\n\\nReflection 3: In the convergence phase, I was able to synthesize the insights from all three personas to arrive at a final answer. I am confident in this answer, as it is based on current scientific knowledge and takes into account the potential societal and technological implications of Mars colonization.\\n\\nReflection 4: The key takeaway from this reasoning process is the importance of considering multiple perspectives when addressing complex questions. In future, I would aim to provide more specific details in my initial thoughts and to consider the potential implications more thoroughly.\\n\\nHistorian Persona: \\n\\nReflection 1: The interaction between the personas contributed to a more holistic understanding of the topic. The Scientist's focus on empirical data and the Optimist's emphasis on potential benefits complemented the historical analysis. However, the discussions sometimes lacked depth, which could be improved in future iterations.\\n\\nReflection 2: The system self-regulated effectively, with each persona considering and responding to the critiques of the others. This led to a more nuanced analysis and helped to identify and address gaps in reasoning. The system's adaptability was demonstrated by the shifts in focus in response to feedback.\\n\\nReflection 3: In the convergence phase, I was able to synthesize the insights from all three personas to arrive at a final answer. I am confident in this answer, as it is grounded in historical context and considers the potential societal and technological implications of Mars colonization.\\n\\nReflection 4: The key takeaway from this reasoning process is the importance of drawing on historical context when addressing complex questions. In future, I would aim to delve deeper into the historical parallels and to consider the potential implications more thoroughly.\\n\\nOptimist Persona: \\n\\nReflection 1: The interaction between the personas contributed to a more balanced understanding of the topic. The Scientist's focus on empirical data and the Historian's emphasis on historical context added valuable perspective to the optimistic analysis. However, the discussions sometimes lacked specificity, which could be improved in future iterations.\\n\\nReflection 2: The system self-regulated effectively, with each persona considering and responding to the critiques of the others. This led to a more nuanced analysis and helped to identify and address gaps in reasoning. The system's adaptability was demonstrated by the shifts in focus in response to feedback.\\n\\nReflection 3: In the convergence phase, I was able to synthesize the insights from all three personas to arrive at a final answer. I am confident in this answer, as it emphasizes the potential benefits and opportunities of Mars colonization, while also considering the potential challenges and implications.\\n\\nReflection 4: The key takeaway from this reasoning process is the importance of maintaining a positive perspective while also considering the potential challenges and implications. In future, I would aim to provide more specific details in my initial thoughts and to consider the potential implications more thoroughly.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Graph-of-Thoughts**"
      ],
      "metadata": {
        "id": "46DXedY1Bl-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graph_of_thoughts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLty1rQOCQ6O",
        "outputId": "618d6868-dfbd-403b-8973-0e053af8e56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting graph_of_thoughts\n",
            "  Downloading graph_of_thoughts-0.0.2-py3-none-any.whl (28 kB)\n",
            "Collecting accelerate>=0.21.0 (from graph_of_thoughts)\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (2.2.1)\n",
            "Collecting bitsandbytes>=0.41.0 (from graph_of_thoughts)\n",
            "  Downloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (1.24.3)\n",
            "Requirement already satisfied: openai>=0.27.7 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (0.27.7)\n",
            "Requirement already satisfied: pandas>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (2.0.3)\n",
            "Requirement already satisfied: scipy>=1.10.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (1.11.3)\n",
            "Requirement already satisfied: sympy>=1.12 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (1.12)\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from graph_of_thoughts) (2.1.0+cu118)\n",
            "Collecting transformers>=4.31.0 (from graph_of_thoughts)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->graph_of_thoughts) (6.0.1)\n",
            "Collecting huggingface-hub (from accelerate>=0.21.0->graph_of_thoughts)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.1->graph_of_thoughts) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.7->graph_of_thoughts) (3.8.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->graph_of_thoughts) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.3->graph_of_thoughts) (2023.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.12->graph_of_thoughts) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->graph_of_thoughts) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->graph_of_thoughts) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.31.0->graph_of_thoughts)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.31.0->graph_of_thoughts)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.1->graph_of_thoughts) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai>=0.27.7->graph_of_thoughts) (2023.5.7)\n",
            "Collecting huggingface-hub (from accelerate>=0.21.0->graph_of_thoughts)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai>=0.27.7->graph_of_thoughts) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->graph_of_thoughts) (2.1.3)\n",
            "Installing collected packages: bitsandbytes, safetensors, huggingface-hub, tokenizers, accelerate, transformers, graph_of_thoughts\n",
            "Successfully installed accelerate-0.23.0 bitsandbytes-0.41.1 graph_of_thoughts-0.0.2 huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spcl/graph-of-thoughts.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBg1iIE4CS7p",
        "outputId": "becf4f99-9d63-494e-d3c4-e8f7de4c5876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'graph-of-thoughts' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "SIagnJTNHdlF",
        "outputId": "20108db2-a633-429e-8f4a-e82f51eabded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Collecting openai\n",
            "  Using cached openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.27.7\n",
            "    Uninstalling openai-0.27.7:\n",
            "      Successfully uninstalled openai-0.27.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tree-of-thoughts-llm 0.1.0 requires openai==0.27.7, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "openai"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the GoT prompt\n",
        "prompt = \"\"\"\n",
        "Graph:\n",
        "  Node 1: {role: \"system\", content: \"Write a poem about a cat.\"}\n",
        "  Node 2: {role: \"user\", content: \"The poem should be about a cat's love of chasing after butterflies.\"}\n",
        "  Node 3: {role: \"assistant\", content: \"The poem should be in the form of a haiku.\"}\n",
        "  Node 4: {role: \"assistant\", content: \"The poem should use vivid imagery to describe the cat's chase.\"}\n",
        "  Node 5: {role: \"assistant\", content: \"The poem should be creative and engaging.\"}\n",
        "\n",
        "Edges:\n",
        "  Node 1 -> Node 2\n",
        "  Node 2 -> Node 3\n",
        "  Node 3 -> Node 4\n",
        "  Node 4 -> Node 5\n",
        "\n",
        "Output:\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the GoT prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-oFRzGVGsGg",
        "outputId": "a1a1f377-3c6d-4b90-c226-38c9494a03a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "The sun shines bright,\n",
            "My cat follows her dreams,\n",
            "Chasing the butterflies in flight.\n",
            "\n",
            "Furry body flowing gracefully,\n",
            "Sweet smelling roses and birds in harmony.\n",
            "Paw steps ringing, like singing harmony.\n",
            "\n",
            "As the wind carries the butterflies away,\n",
            "My cat's love for chase never fades away.\n",
            "Bright eyes sparkles as she hunts her prey,\n",
            "A beautiful sight come out to play.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Algorithm-of-Thoughts**"
      ],
      "metadata": {
        "id": "r_9hMV7UBpTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the AoT task\n",
        "task = \"\"\"\n",
        "Generate Python code for the sum of n natural numbers.\n",
        "\n",
        "Background: The sum of n natural numbers is given by the formula (n*(n+1))/2.\n",
        "\n",
        "Initial hypothesis: The Python code should use a for loop to iterate from 1 to n, and add each number to the sum.\n",
        "\n",
        "Reasoning: The for loop will ensure that all of the natural numbers from 1 to n are included in the sum.\n",
        "\n",
        "Conclusion: The Python code should return the sum of all of the natural numbers from 1 to n.\n",
        "\n",
        "Objective: Generate Python code that meets all of the above requirements, and that is efficient and idiomatic.\n",
        "\"\"\"\n",
        "\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=task,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000\n",
        ")\n",
        "\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg5W9tBrKoVc",
        "outputId": "4e882c2a-0b69-492b-cb09-0501d3d43998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Solution:\n",
            "\n",
            "def sum_of_n_numbers(n):\n",
            "    sum = 0\n",
            "    for num in range(1, n + 1):\n",
            "        sum += num\n",
            "    return sum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**RASCEF**"
      ],
      "metadata": {
        "id": "_AdHuWVVBsu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the RASCEF prompt\n",
        "prompt = \"\"\"\n",
        "Role: AI as a financial advisor.\n",
        "Action: Create a detailed monthly budget plan for a startup entrepreneur in the early growth stage, with the goal of increasing revenue by 20% and reducing expenses by 10%.\n",
        "Steps:Identify revenue and expenses, set savings goals, and allocate funds.\n",
        "Context: The startup has been operating for 6 months and has a revenue of $10,000 per month. The startup's current expenses are $8,000 per month.\n",
        "Examples:The following is a list of the startup's current revenue and expense items: Revenue: Sales of widgets: $10,000 Expenses:Marketing: $2,000 Rent: $1,500 Salaries: $3,000 Other expenses: $1,500\n",
        "Format: Based on this write it as a detailed monthly budget plan.\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the RASCEF prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=200  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated text\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBHF1AuSMeW7",
        "outputId": "b98d8d7e-d18c-4cc7-9067-06662b3c88ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Detailed Monthly Budget Plan for Startup Entrepreneur\n",
            "Funds Allocation: \n",
            "1. Savings Goals: \n",
            "Save 10% of total income for emergencies and future investments. \n",
            "2. Revenue: \n",
            "Sales of widgets: $10,000 \n",
            "3.Expenses: \n",
            "Marketing: Allocate $2200 of income for marketing expenses, an increase of 10% from the existing $2,000. \n",
            "Rent: Allocate $1650 for rent, an increase of 10% from the existing $1,500. \n",
            "Salaries: Allocate $3300 for salaries, an increase of 10% from the existing $3,000. \n",
            "Other expenses: Allocate $1650 for other expenses, a decrease of 10% from the existing $1,500. \n",
            "Summary: This budget plan aims to increase revenue by 10%, as well as reducing expenses by 10%, in order to reach the goal of achieving a 20% growth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**REACT**"
      ],
      "metadata": {
        "id": "782ppc0EBvam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Define the ReAct-format prompt exemplar\n",
        "prompt = \"\"\"\n",
        "Question: What is the elevation range for the area that the eastern sector of the\n",
        "Colorado orogeny extends into?\n",
        "\n",
        "Thought 1: I need to search Colorado orogeny, find the area that the eastern sector\n",
        "of the Colorado orogeny extends into, then find the elevation range of the\n",
        "area.\n",
        "\n",
        "Action 1: Search[Colorado orogeny]\n",
        "\n",
        "Observation 1: The Colorado orogeny was an episode of mountain building (an orogeny) in\n",
        "Colorado and surrounding areas.\n",
        "\n",
        "Thought 2: It does not mention the eastern sector. So I need to look up eastern\n",
        "sector.\n",
        "\n",
        "Action 2: Lookup[eastern sector]\n",
        "\n",
        "Observation 2: (Result 1 / 1) The eastern sector extends into the High Plains and is called\n",
        "the Central Plains orogeny.\n",
        "\n",
        "Thought 3: The eastern sector of Colorado orogeny extends into the High Plains. So I\n",
        "need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3: Search[High Plains]\n",
        "\n",
        "Observation 3: High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4: I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4: Search[High Plains (United States)]\n",
        "\n",
        "Observation 4: The High Plains are a subregion of the Great Plains. From east to west, the\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130\n",
        "m).[3]\n",
        "\n",
        "Thought 5: High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer\n",
        "is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5: Finish[1,800 to 7,000 ft]\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response using the ReAct-format prompt\n",
        "response = openai.Completion.create(\n",
        "    engine=\"text-davinci-003\",  # Use the appropriate engine/model\n",
        "    prompt=prompt,\n",
        "    api_key=api_key,\n",
        "    max_tokens=1000  # Adjust as needed\n",
        ")\n",
        "\n",
        "# Get the response from the model\n",
        "reply = response.choices[0].text\n",
        "\n",
        "# Print the generated text\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "w2i-VgiOMwXb",
        "outputId": "5ffeb47e-f0be-4caf-d64e-7d97c903de37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e39ceb3b1359>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define the ReAct-format prompt exemplar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt = \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0mQuestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mWhat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melevation\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marea\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0meastern\u001b[0m \u001b[0msector\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n6sh6UykOVAZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}